---
layout: default
---

## Foysal Ahmed

**PhD Student**  
**Donghua University (211, Double First-Class) ¬∑ Shanghai, China** <br>

üìß **Email:** [foysal.9@outlook.com](mailto:foysal.9@outlook.com)  <br>
üåê **GitHub:** [github.com/foysalahmed10](https://github.com/foysalahmed10)  <br>
üìÑ **CV:** [Download CV](cv.pdf)  <br>
üéì **Google Scholar:** [Google Scholar Profile](https://scholar.google.com/citations?user=1HUchzEAAAAJ&hl=en)

---

## Academic History and Education Background

I received my **Bachelor‚Äôs degree in Software Engineering** from **Daffodil International University**, Bangladesh, in **2018**. In **September 2019**, I was awarded the **Chinese Government Scholarship** and moved to China to pursue graduate studies. I completed my **Master‚Äôs degree in Information and Communication Engineering** at **Donghua University**, Shanghai, China, in **2022**. Following my master‚Äôs studies, I was awarded the **Shanghai Government Scholarship** to pursue my **PhD** at **Donghua University**. I am currently a PhD student in the **Information and Communication Intelligent Systems** program, where my research focuses on data-driven methods for **3D plant phenotyping, 3D point cloud processing, generative modeling and computer vision**.


---

## Honors and Awards

- **Shanghai Government Scholarship** for PhD Study, China  
  Awarded for doctoral study at Donghua University.

- **Chinese Government Scholarship** for Master‚Äôs Study, China  
  Awarded for full-time master‚Äôs study at Donghua University.

- **Excellent International Graduate Award**, Donghua University, China, 2022  
  Awarded in recognition of outstanding academic performance and overall excellence during master‚Äôs study.

- **Outstanding International Student Award**, Donghua University, China, 2021  
  Awarded for excellent academic performance and active participation in university activities.

- **Certificate of Honor for Outstanding Performance**,  
  International Cultural Exchange School, Donghua University, 2019  
  Awarded for outstanding performance during the International Cultural Festival of Donghua University.

- **Certificate of Merit for Social Work**,  
  International Cultural Exchange School, Donghua University, Spring 2023  
  Awarded in recognition of contributions to social and cultural activities.

---

## Research Interests
- 3D Plant Phenotyping  
- Point Cloud Processing and Learning  
- Deep Learning for Agriculture  
- Generative Models (GANs, Diffusion Models)  

---

## Upcoming & Ongoing Projects

### üå∂Ô∏è Pepper-4D ‚Äî 4D Plant Point Cloud Dataset

<p align="center">
  <img src="assets/graphical_abstract_pepper_4d.jpg" width="95%">
</p>

**Resources**  
- üìÑ **Paper:** to be published
- üíª **Code:** [https://github.com/  ](https://github.com/foysalahmed10/Pepper-4D)

**Introduction** <br>

Three-dimensional (3D) plant phenotyping plays a crucial role in analyzing
plant structure, organ-level traits, and growth dynamics.
While recent advances in 2D and 3D vision have accelerated research in plant
phenotyping, publicly available **spatiotemporal 3D datasets for pepper plants**
remain extremely limited.

Pepper-4D addresses this gap by providing a **comprehensive 4D (3D + time) point
cloud dataset** capturing the complete growth and developmental process of
pepper plants under controlled indoor conditions.
The dataset enables detailed analysis of plant structure, organ dynamics,
temporal growth patterns, and plant health status.
Pepper-4D is designed as a benchmark resource for both discriminative and
generative learning methods on 3D plant point clouds.


**Dataset Overview**  <br>

- **Species:** Pepper (*Capsicum annuum*)
- **Total plants:** 29
- **Total point clouds:** 916
- **Total points:** 322.72 million
- **Total dataset size:** ~20 GB
- **Data modality:** 3D point clouds (XYZ)
- **Temporal resolution:** Bi-daily scans
- **Environment:** Indoor controlled conditions


**Comparison with Existing 3D Crop Datasets**  <br>

The following table provides a high-level comparison between Pepper-4D and
representative publicly available 3D crop datasets.

<table align="center">
  <thead>
    <tr>
      <th align="center">Dataset</th>
      <th align="center">Species</th>
      <th align="center"># Point Clouds</th>
      <th align="center"># Temporal Coverage (4D)</th>
      <th align="center">Acquisition Modality</th>
      <th align="center">Color</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td align="center">Conn et al. (2017)</td>
      <td align="center">Tomato, Tobacco, Sorghum</td>
      <td align="center">558</td>
      <td align="center">Yes</td>
      <td align="center">Laser</td>
      <td align="center">No</td>
    </tr>
    <tr>
      <td align="center">ROSE-X (2020)</td>
      <td align="center">Rose</td>
      <td align="center">11</td>
      <td align="center">No</td>
      <td align="center">X-ray CT</td>
      <td align="center">No</td>
    </tr>
    <tr>
      <td align="center">Pheno4D (2021)</td>
      <td align="center">Maize, Tomato</td>
      <td align="center">126</td>
      <td align="center">Yes</td>
      <td align="center">Laser</td>
      <td align="center">No</td>
    </tr>
    <tr>
      <td align="center">Soybean-MVS (2023)</td>
      <td align="center">Soybean</td>
      <td align="center">102</td>
      <td align="center">Yes</td>
      <td align="center">MVS</td>
      <td align="center">Yes</td>
    </tr>
    <tr>
      <td align="center">PLANesT-3D (2024)</td>
      <td align="center">Pepper, Rose, Ribes</td>
      <td align="center">34</td>
      <td align="center">No</td>
      <td align="center">SfM-MVS</td>
      <td align="center">Yes</td>
    </tr>
    <tr>
      <td align="center">BonnBeetClouds3D (2024)</td>
      <td align="center">Sugar Beet</td>
      <td align="center">3000</td>
      <td align="center">No</td>
      <td align="center">UAV Photogrammetry</td>
      <td align="center">Yes</td>
    </tr>
    <tr>
      <td align="center">Crops3D (2024)</td>
      <td align="center">Multiple species</td>
      <td align="center">1230</td>
      <td align="center">Partial</td>
      <td align="center">SfM-MVS, TLS</td>
      <td align="center">Yes</td>
    </tr>
    <tr>
      <td align="center">MaizeField3D (2025)</td>
      <td align="center">Maize</td>
      <td align="center">1045</td>
      <td align="center">No</td>
      <td align="center">TLS + Procedural</td>
      <td align="center">Yes</td>
    </tr>
    <tr>
      <td align="center"><b>Pepper-4D (2026)</b></td>
      <td align="center"><b>Pepper</b></td>
      <td align="center"><b>916</b></td>
      <td align="center"><b>Yes</b></td>
      <td align="center"><b>SfM + NeRF</b></td>
      <td align="center"><b>Yes</b></td>
    </tr>
  </tbody>
</table>



**Dataset Acquisition and Reconstruction**  <br>

<p align="center">
  <img src="assets/data_acquisition_pipeline.jpg" width="95%">
</p>

Pepper-4D was constructed through a multi-stage pipeline including image
acquisition, preprocessing, 3D reconstruction, and plant-only point cloud
generation. Multi-view images were captured for each plant at each time step,
followed by 3D reconstruction and post-processing to obtain temporally aligned
point clouds.


**Dataset Subsets**  <br>

Pepper-4D consists of three subsets capturing different growth scenarios and
phenotyping tasks.

**Subset 1 ‚Äî Full Lifecycle**  <br>

<p align="center">
  <img src="assets/subset1_lifecycle.jpg" width="95%">
</p>

- **Plants:** 11  
- **Point clouds:** 460  
- **Description:**  
  Long-term monitoring from early vegetative growth to flowering,
  fruiting, and senescence.
- **Annotations:**  
  Semantic, instance, temporal, and health labels.


**Subset 2 ‚Äî Geotropism Tests**   <br>

<p align="center">
  <img src="assets/subset2_new_organs.jpg" width="95%">
</p>

- **Plants:** 8  
- **Point clouds:** 238  
- **Description:**  
  Sequences focusing on **new organ detection and Geotropism tests**.
- **Annotations:**  
  New organ detection labels.


**Subset 3 ‚Äî Early‚ÄìMid Growth**  <br>

- **Plants:** 10  
- **Point clouds:** 218  
- **Description:**  
  Early and mid growth stages with rapid structural changes.
- **Annotations:**  
  None 


---

## Publications (Selected)

### 3D-NOD: 3D New Organ Detection in Plant Growth by a Spatiotemporal Point Cloud Deep Segmentation Framework

Dawei Li‚Ä†, **Foysal Ahmed**‚Ä†, Z. Wang‚Ä† <br>
<strong>Plant Phenomics</strong>, 2025.<br>
‚Ä† Contributed equally.

**Brief Description:**  
This paper presents a spatiotemporal deep learning framework for detecting newly emerged plant organs from 4D point cloud sequences.  
By jointly modeling spatial structure and temporal growth dynamics, the proposed method enables accurate and robust organ-level analysis during plant development.

**Resources**  
- üìÑ **Paper:** [https://www.sciencedirect.com](https://doi.org/10.1016/j.plaphe.2025.100002)/  
- üíª **Code:** [https://github.com/  ](https://github.com/foysalahmed10/3D-New-Organ-Detection-in-Plant-Growth-from-Spatiotemporal-Point-Clouds)

<table border="0">
  <tr>
    <td align="center">
      <img src="assets/3D-NOD1.gif" width="85%" />
    </td>
  </tr>
</table>
<table border="0">
  <tr>
    <td align="center">
      <img src="assets/3D-NOD2.jpg" width="85%" />
    </td>
  </tr>
</table>
<table border="0">
  <tr>
    <td align="center">
      <img src="assets/3D-NOD3.jpg" width="85%" />
    </td>
  </tr>
</table>


---

### A Novel Knowledge Distillation Framework for Enhancing Small Object Detection in Blurry Environments with UAV-Assisted Images

Sayed Jobaer, Xue-song Tang, Yihong Zhang, Gaojian Li, **Foysal Ahmed**  
<strong>Complex & Intelligent Systems</strong>, 2025.  

**Brief Description:**  
This paper proposes a novel knowledge distillation framework to enhance small object detection performance in blurry UAV-assisted images.  
The method integrates attention-guided feature distillation and a feature-level deblurring subnet within a self-supervised learning paradigm, enabling robust detection without relying on explicit image deblurring. Experimental results on VisDrone and a newly developed UAV small object detection dataset demonstrate consistent performance improvements over state-of-the-art methods.

**Resources**  
- üìÑ **Paper:** [https://doi.org/10.1007/s40747-024-01676-w  ](https://link.springer.com/article/10.1007/s40747-024-01676-w)
- üíª **Code:** *(to be released)*  

<table border="0">
  <tr>
    <td align="center">
      <img src="assets/uav_kd_framework_1.png" width="85%" />
      <br>
      <em>Overview of the proposed knowledge distillation and feature-level deblurring framework.</em>
    </td>
  </tr>
</table>
<table border="0">
  <tr>
    <td align="center">
      <img src="assets/uav_kd_framework_2.png" width="85%" />
      <br>
      <em>Overview of the proposed knowledge distillation and feature-level deblurring framework.</em>
    </td>
  </tr>
</table>

---


### YOLO-JD: A Deep Learning Network for Jute Diseases and Pests Detection

Dawei Li‚Ä†, **Foysal Ahmed**‚Ä†, N. Wu, and A. I. Sethi<br>
<strong>Plants</strong>, 2022.<br>
‚Ä† Contributed equally.

**Brief Description:**  
This work proposes a YOLO-based detection framework for identifying jute diseases and pests from field images, enabling practical and efficient disease monitoring in real agricultural environments.

**Resources**  
- üìÑ **Paper:** [https://doi.org/10.3390/plants11070937 ](https://doi.org/10.3390/plants11070937) 
- üíª **Code:** [https://github.com/foysalahmed10/YOLO-JD  ](https://github.com/foysalahmed10/YOLO-JD)

<table border="0">
  <tr>
    <td align="center">
      <img src="assets/Jute_1.jpg" width="85%" />
      <br>
      <em>The overall architecture of YOLO-JD.</em>
    </td>
  </tr>
</table>


<table border="0">
  <tr>
    <td align="center">
      <img src="assets/Jute_2.jpg" width="85%" />
      <br>
      <em>YOLO-JD predictions on complex background images.</em>
    </td>
  </tr>
</table>


---

### UAV-Assisted Hybrid Scheme for Urban Road Safety Based on VANETs

Sayed Jobaer, Yihong Zhang*, Muhammad Ather Iqbal Hussain, **Foysal Ahmed**<br>
<strong>Electronics</strong>, 2020.<br>
*Corresponding author.*

**Brief Description:**  
This paper proposes a UAV-assisted hybrid routing framework for urban vehicular ad hoc networks (VANETs) to improve road safety and congestion control. By integrating unmanned aerial vehicles (UAVs) with roadside units and vehicular communication, the proposed scheme enhances packet delivery ratio, reduces end-to-end delay, and improves energy efficiency under realistic urban traffic conditions.

**Resources**  
- üìÑ **Paper:** [https://www.mdpi.com/2079-9292/9/9/1499](https://doi.org/10.3390/electronics9091499)
- üíª **Code:** *(to be released)*  

<table border="0">
  <tr>
    <td align="center">
      <img src="assets/uav_vanet_1.png" width="80%" />
      <br>
      <em>UAV-assisted VANET architecture for urban traffic congestion control.</em>
    </td>
  </tr>
</table>


---


<hr>

<!-- Visitor Statistics (Footer) -->
<div class="site-stats">

  <!-- Page View Counter -->
  <p class="page-views">
    Page views: <span id="busuanzi_value_site_pv">‚Äî</span>
  </p>

  <!-- Visitor Map -->
  <div class="visitor-map">
    <script 
      type="text/javascript" 
      id="mmvst_globe" 
      src="//mapmyvisitors.com/globe.js?d=_hWUodtZRAlm6HykAXB9pO3DOpUUhQQ7sCp_kY9ZW00">
    </script>
  </div>

</div>

<!-- Busuanzi Counter Script -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<style>
/* Footer visitor statistics */
.site-stats {
  margin-top: 2.5em;
  text-align: center;
  font-size: 0.9em;
  color: #666;
}

/* Page view text */
.site-stats .page-views {
  margin-bottom: 1em;
}

/* Visitor map container */
.site-stats .visitor-map {
  max-width: 600px;
  margin: 0 auto;
}

/* Make MapMyVisitors globe responsive */
#mmvst_globe,
#mmvst_globe iframe,
#mmvst_globe canvas {
  width: 100% !important;
  max-width: 100% !important;
}
</style>
